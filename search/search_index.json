{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>OntoGPT is a Python package for extracting structured information from text with large language models (LLMs), instruction prompts, and ontology-based grounding. It works well with OpenAI's GPT-3.5 and GPT-4 models as well as a selection of other LLMs. OntoGPT's output can be used for general-purpose natural language tasks (e.g., named entity recognition and relation extraction), summarization, knowledge base and knowledge graph construction, and more.</p>"},{"location":"#methods","title":"Methods","text":"<p>Two different strategies for knowledge extraction are currently implemented in OntoGPT:</p> <ul> <li>SPIRES: Structured Prompt Interrogation and Recursive Extraction of Semantics</li> <li>A Zero-shot learning (ZSL) approach to extracting nested semantic structures from text</li> <li>This approach takes two inputs - 1) LinkML schema 2) free text, and outputs knowledge in a structure conformant with the supplied schema in JSON, YAML, RDF or OWL formats</li> <li>Uses GPT-3.5-turbo, GPT-4, or one of a variety of open LLMs on your local machine</li> <li>SPINDOCTOR: Structured Prompt Interpolation of Narrative Descriptions Or Controlled Terms for Ontological Reporting</li> <li>Summarizes gene set descriptions (pseudo gene-set enrichment)</li> <li>Uses GPT-3.5-turbo or GPT-4</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Please see the Setup page on the left for more detailed instructions.</p> <p>OntoGPT runs on the command line, though there's also a minimal web app interface (see <code>Web Application</code> section below).</p> <ol> <li>Ensure you have Python 3.9 or greater installed.</li> <li> <p>Install with <code>pip</code>:</p> <p><code>bash pip install ontogpt</code></p> </li> <li> <p>Set your OpenAI API key:</p> <p><code>bash runoak set-apikey -e openai &lt;your openai api key&gt;</code></p> </li> <li> <p>See the list of all OntoGPT commands:</p> <p><code>bash ontogpt --help</code></p> </li> <li> <p>Try a simple example of information extraction:</p> <p><code>bash echo \"One treatment for high blood pressure is carvedilol.\" &gt; example.txt ontogpt extract -i example.txt -t drug</code></p> <p>OntoGPT will retrieve the necessary ontologies and output results to the command line. Your output will provide all extracted objects under the heading <code>extracted_object</code>.</p> </li> </ol>"},{"location":"#web-applications","title":"Web Applications","text":"<p>There is a bare bones web application for running OntoGPT and viewing results.</p> <p>First, install the required dependencies with <code>pip</code> by running the following command:</p> <pre><code>pip install ontogpt[web]\n</code></pre> <p>Then run this command to start the web application:</p> <pre><code>web-ontogpt\n</code></pre> <p>NOTE: We do not recommend hosting this webapp publicly without authentication.</p> <p>Gene enrichment has its own webapp powered by Streamlit:</p> <pre><code>streamlit run src/ontogpt/streamlit/spindoctor.py\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>SPIRES is described further in: Caufield JH, Hegde H, Emonet V, Harris NL, Joachimiak MP, Matentzoglu N, et al. Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. arXiv publication: http://arxiv.org/abs/2304.02711</p> <p>SPINDOCTOR is described further in: Joachimiak MP, Caufield JH, Harris NL, Kim H, Mungall CJ. Gene Set Summarization using Large Language Models. arXiv publication: http://arxiv.org/abs/2305.13338</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! One way to get started with contributing to OntoGPT is to submit an </p> <p>Contributions on recipes to test welcome from anyone! Just make a PR here. See this list for accepted URLs</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We gratefully acknowledge Bosch Research for their support of this research project.</p>"},{"location":"custom/","title":"Custom Schemas","text":""},{"location":"custom/#build-a-custom-schema","title":"Build a custom schema","text":"<p>Define a schema (using a subset of LinkML) that describes the structure in which you want to extract knowledge from your text.</p> <p>There are a number of pre-defined LinkML data models already developed here - src/ontogpt/templates/ which you can use as reference when creating your own data models.</p>"},{"location":"custom/#the-header","title":"The header","text":"<p>The header of the schema defines metadata, parameters instructing LinkML to interpret prefixes in specific ways, and names of imports.</p> <p>You will find these fields in the schema header:</p> <ul> <li><code>id</code>: A unique identifier for the schema. These may take the form of W3IDs like <code>http://w3id.org/ontogpt/schemaname</code>.</li> <li><code>name</code>: Name of the schema. Should resemble the filename.</li> <li><code>title</code>: Title of the schema for human readability.</li> <li><code>description</code>: A human readable description of the schema. Detail is welcome!</li> <li><code>license</code>: A license indicating reusability of the schema. We prefer a CC0 license.</li> <li><code>prefixes</code>: This is a list (actually a dictionary, as each item contains a key and a value) of short prefixes for the identifier namespaces used in the schema along with their corresponding identifier prefixes. At minimum, this shoulc include <code>linkml: https://w3id.org/linkml/</code> and the prefix for your schema itself. If the schema is named <code>salmon</code>, this will be something like <code>salmon: http://w3id.org/ontogpt/salmon/</code>. If you're using Gene Ontology identifiers, specify their prefixes as <code>GO: http://purl.obolibrary.org/obo/GO_</code>, because each Gene Ontology term has an identifier prefixed with <code>GO:</code>.</li> <li><code>default_prefix</code>: This is the prefix for your schema. It should match what you used in the <code>prefixes</code> list, so following the above example, that would be <code>salmon</code>.</li> <li> <p><code>default_range</code>: This is the default data type LinkML will assume each class should be unless specified otherwise. Using <code>string</code> is usually safe.</p> </li> <li> <p><code>imports</code>: A list of other schemas to import types from. These may be any schemas in the same directory (e.g., if you have another schema named <code>francine</code> then you may simply include <code>francine</code> in the list of imports) but note that LinkML will raise an error if multiple classes have the same name across the imports. Minimally, this list should include <code>linkml:types</code> and <code>core</code> to import the base LinkML data types and the generic OntoGPT classes, respectively.</p> </li> </ul> <p>An example:</p> <pre><code>id: http://w3id.org/ontogpt/gocam\nname: gocam-template\ntitle: GO-CAM Template\ndescription: &gt;-\n  A template for GO-CAMs\nlicense: https://creativecommons.org/publicdomain/zero/1.0/\nprefixes:\n  linkml: https://w3id.org/linkml/\n  gocam: http://w3id.org/ontogpt/gocam/\n  GO: http://purl.obolibrary.org/obo/GO_\n  CL: http://purl.obolibrary.org/obo/CL_\n\ndefault_prefix: gocam\ndefault_range: string\n\nimports:\n  - linkml:types\n  - core\n</code></pre>"},{"location":"custom/#the-classes","title":"The classes","text":"<p>The classes in the schema define the \"things\" you are interested in extracting. LinkML doesn't make many assumptions about the difference between a class and a relationship, a node and an edge, or a relation and a property. It's designed to be flexibile enough to handle a variety of data models.</p> <p>The start of this section is indicated by <code>classes:</code>.</p> <p>A minimal class may look like this:</p> <pre><code>  ClassName:\n    is_a: NamedEntity\n    attributes:\n      entity:\n        range: string\n        description: &gt;- \n          A named entity.\n</code></pre> <p>In practice, this class won't do much, as it doesn't contain much for OntoGPT to work with or many instructions to form an LLM prompt out of. And that's fine, because we can do more.</p> <p>These fields may be used in classes:</p> <ul> <li><code>is_a</code>: This describes a hierarchical structure, so the value of this slot is the name of a LinkML class. <code>NamedEntity</code> is defined in OntoGPT's core schema and is will ensure extracted objects of this class have both unique identifiers and human readable labels.</li> <li><code>tree_root</code>: If <code>true</code>, this class will be treated as the root of the data hierarchy. If you're planning to extract specific objects from a full text document, for example, it may be useful to define a class for the document to contain its metadata. This parent class could then be the <code>tree_root</code>.</li> <li><code>attributes</code>: This slot defines all class attributes, and in OntoGPT, that means each will be included in a prompt for the LLM. Each attribute should have a unique, lowercased name. Attributes have their own slots:</li> <li><code>description</code>: The attribute description to be passed as part of the prompt. This should describe the attribute and how it should be formatted in the generated output. Do not include references to specific identifiers here.</li> <li><code>multivalued</code>: If <code>true</code>, any value for this attribute will be interpreted as a list. This is cruical if you expect multiple values in the extracted output and should be reflected in the description by indicating how each value should be separated. OntoGPT prefers semicolons.</li> <li><code>range</code>: The class to restrict the object to. This may be an abstract data type like <code>string</code> or another class defined elsewhere in your schema, like <code>Gene</code> in the example below.</li> <li><code>id_prefixes</code>: A list of identifiers to ground values of this class to. Usually specific to a class rather than an attribute. Use capitalized forms and omit the colon. If you want to ground to MeSH terms, for example, include the prefix <code>MESH</code>.</li> <li><code>annotations</code>: This slot contains specific instructions for OntoGPT in its annotation and grounding operations. The heading <code>annotators</code>, placed under this slot, must contain a comma separated list of value annotators provided by the Ontology Access Kit (OAK). In OAK these are called implementations or adapters and there are many of them available.. Annotators are responsible for bridging the gap between raw text and unique identifier, though that process may involve searching a combination of term lists along with their synonyms and equivalents.</li> <li>OBO Foundry ontologies make great annotators. To use CHEBI for chemical names, for example, use the annotator <code>sqlite:obo:chebi</code> and include <code>CHEBI</code> in the <code>id_prefixes</code> list.</li> <li>Ontologies in BioPortal work well, too. They may be specified with the BioPortal ID. To use the EnvThes ecological thesaurus, for example, use the annotator <code>bioportal:ENVTHES</code> and the prefix <code>ENVTHES</code>.</li> <li><code>slot_usage</code>: This slot can contain rules about how another slot may be restricted. In the example below, <code>GeneLocation</code> has values for its <code>id</code> slot restricted to values within two different enums. See the next section for more information on how to use enums.</li> </ul> <p>An example, continuing from where the header left off:</p> <pre><code>classes:\n  GoCamAnnotations:\n    tree_root: true\n    attributes:\n      genes:\n        description: semicolon-separated list of genes\n        multivalued: true\n        range: Gene\n      organisms:\n        description: semicolon-separated list of organism taxons\n        multivalued: true\n        range: Organism\n      gene_organisms:\n        annotations:\n          prompt: semicolon-separated list of asterisk separated gene to organism relationships\n        multivalued: true\n        range: GeneOrganismRelationship\n      activities:\n        description: semicolon-separated list of molecular activities\n        multivalued: true\n        range: MolecularActivity\n      gene_functions:\n        description: semicolon-separated list of gene to molecular activity relationships\n        multivalued: true\n        range: GeneMolecularActivityRelationship\n      cellular_processes:\n        description: semicolon-separated list of cellular processes\n        multivalued: true\n        range: CellularProcess\n      pathways:\n        description: semicolon-separated list of pathways\n        multivalued: true\n        range: Pathway\n      gene_gene_interactions:\n        description: semicolon-separated list of gene to gene interactions\n        multivalued: true\n        range: GeneGeneInteraction\n      gene_localizations:\n        description: &gt;-\n          semicolon-separated list of genes plus their location in the cell;\n          for example, \"gene1 / cytoplasm; gene2 / mitochondrion\"\n        multivalued: true\n        range: GeneSubcellularLocalizationRelationship\n\n  Gene:\n    is_a: NamedEntity\n    id_prefixes:\n      - HGNC\n      - PR\n      - UniProtKB\n    annotations:\n      annotators: gilda:, bioportal:hgnc-nr\n  Pathway:\n    is_a: NamedEntity\n    id_prefixes:\n      - GO\n      - PW\n    annotations:\n      annotators: sqlite:obo:go, sqlite:obo:pw\n  CellularProcess:\n    is_a: NamedEntity\n    id_prefixes:\n      - GO\n    annotations:\n      annotators: sqlite:obo:go\n  MolecularActivity:\n    is_a: NamedEntity\n    id_prefixes:\n      - GO\n    annotations:\n      annotators: sqlite:obo:go\n  GeneLocation:\n    is_a: NamedEntity\n    id_prefixes:\n      - GO\n      - CL\n      - UBERON\n    annotations:\n      annotators: \"sqlite:obo:go, sqlite:obo:cl\"\n    slot_usage:\n      id:\n        values_from:\n          - GOCellComponentType\n          - CellType\n  Organism:\n    is_a: NamedEntity\n    id_prefixes:\n      - NCBITaxon\n      - EFO\n    annotations:\n      annotators: gilda:, sqlite:obo:ncbitaxon\n  Molecule:\n    is_a: NamedEntity\n    id_prefixes:\n      - CHEBI\n      - PR\n    annotations:\n      annotators: gilda:, sqlite:obo:chebi\n\n  GeneOrganismRelationship:\n    is_a: CompoundExpression\n    attributes:\n      gene:\n        range: Gene\n      organism:\n        range: Organism\n\n  GeneMolecularActivityRelationship:\n    is_a:   CompoundExpression\n    attributes:\n      gene:\n        range: Gene\n        annotations:\n          prompt: the name of the gene in the pair. This comes first.\n      molecular_activity:\n        range: MolecularActivity\n        annotations:\n          prompt: the name of the molecular function in the pair. This comes second. May be a GO term.\n\n  GeneMolecularActivityRelationship2:\n    is_a:   CompoundExpression\n    attributes:\n      gene:\n        range: Gene\n        annotations:\n          prompt: the name of the gene.\n      molecular_activity:\n        range: MolecularActivity\n        annotations:\n          prompt: the name of the molecular activity, for example, ubiquitination. May be a GO term.\n      target:\n        range: Molecule\n        annotations:\n          prompt: the name of the molecular entity that is the target of the molecular activity.\n\n  GeneSubcellularLocalizationRelationship:\n    is_a:   CompoundExpression\n    attributes:\n      gene:\n        range: Gene\n      location:\n        range: GeneLocation\n\n  GeneGeneInteraction:\n    is_a:   CompoundExpression\n    attributes:\n      gene1:\n        range: Gene\n      gene2:\n        range: Gene\n</code></pre>"},{"location":"custom/#enums","title":"Enums","text":"<p>LinkML supports defining enums, or sets of values. In OntoGPT this allows schemas to work with subsets of identifiers. Enums have their own hierarchy. In the example below, the <code>reachable_from</code> slot is used to define sets of values: in <code>GOCellComponentType</code> these are all children of the GO term with the ID <code>GO:0005575</code> (cellular component), so restricting a set of identifiers based on this enum will ensure they all correspond to cellular components.</p> <p>Example, starting where the classes left off above:</p> <pre><code>enums:\n\n  GeneLocationEnum:\n    inherits:\n      - GOCellComponent\n      - CellType\n\n  GOCellComponentType:\n    reachable_from:\n      source_ontology: obo:go\n      source_nodes:\n        - GO:0005575 ## cellular_component\n  CellType:\n    reachable_from:\n      source_ontology: obo:cl\n      source_nodes:\n        - CL:0000000 ## cell\n</code></pre>"},{"location":"custom/#schema-design-tips","title":"Schema design tips","text":"<p>It helps to have an understanding of the LinkML schema language, but it should be possible to define your own schemas using the examples in src/ontogpt/templates as a guide.</p> <ul> <li>Prompt hints can be specified using the <code>prompt</code> annotation (otherwise description is used)</li> <li>Multivalued fields are supported</li> <li>The default range is string \u2014 these are not grounded. Ex.: disease name, synonyms</li> <li>Define a class for each <code>NamedEntity</code></li> <li>For any <code>NamedEntity</code>, you can specify a preferred annotator using the <code>annotators</code> annotation</li> </ul> <p>We recommend following an established schema like BioLink Model, but you can define your own.</p> <p>Once you have defined your own schema / data model and placed in the correct directory, you can run the <code>extract</code> command.</p> <p>Ex.:</p> <pre><code>ontogpt extract -t mendelian_disease.MendelianDisease -i marfan-wikipedia.txt\n</code></pre> <p>OntoGPT-specific extensions are specified as annotations.</p> <p>You can specify a set of annotators for a field using the <code>annotators</code> annotation.</p> <p>Ex.:</p> <pre><code>  Gene:\n    is_a: NamedThing\n    id_prefixes:\n      - HGNC\n    annotations:\n      annotators: gilda:, bioportal:hgnc-nr, obo:pr\n</code></pre> <p>The annotators are applied in order.</p> <p>Additionally, when performing grounding, the following measures can be taken to improve accuracy:</p> <ul> <li>Specify the valid set of ID prefixes using <code>id_prefixes</code></li> <li>Some vocabularies have structural IDs that are amenable to regexes, you can specify these using <code>pattern</code></li> <li>You can make use of <code>values_from</code> slot to specify a Dynamic Value Set</li> <li>For example, you can constrain the set of valid locations for a gene product to be subclasses of <code>cellular_component</code> in GO or <code>cell</code> in CL</li> </ul> <p>Ex.:</p> <pre><code>classes:\n  ...\n  GeneLocation:\n    is_a: NamedEntity\n    id_prefixes:\n      - GO\n      - CL\n    annotations:\n      annotators: \"sqlite:obo:go, sqlite:obo:cl\"\n    slot_usage:\n      id:\n        values_from:\n          - GOCellComponentType\n          - CellType\n\nenums:\n  GOCellComponentType:\n    reachable_from:\n      source_ontology: obo:go\n      source_nodes:\n        - GO:0005575 ## cellular_component\n  CellType:\n    reachable_from:\n      source_ontology: obo:cl\n      source_nodes:\n        - CL:0000000 ## cell\n</code></pre>"},{"location":"custom/#multiple-levels-of-nesting","title":"Multiple levels of nesting","text":"<p>Currently no more than two levels of nesting are recommended.</p> <p>If a field has a range which is itself a class and not a primitive, it will attempt to nest.</p> <p>Ex. the <code>gocam</code> schema has an attribute:</p> <pre><code>  attributes:\n      ...\n      gene_functions:\n        description: semicolon-separated list of gene to molecular activity relationships\n        multivalued: true\n        range: GeneMolecularActivityRelationship\n</code></pre> <p>The range <code>GeneMolecularActivityRelationship</code> has been specified inline, so it will nest.</p> <p>The generated prompt is:</p> <pre><code>gene_functions : &lt;semicolon-separated list of gene to molecular activities relationships&gt;\n</code></pre> <p>The output of this is then passed through further SPIRES iterations.</p>"},{"location":"custom/#text-length-limit","title":"Text length limit","text":"<p>LLMs have context sizes limiting the combined length of their inputs and outputs. The <code>gpt-3.5-turbo</code> model, for example, has a 4,096 token limit (prompt + completion), while the <code>gpt-3.5-turbo-16k</code> model has a larger context of 16,384 tokens.</p>"},{"location":"custom/#install-a-custom-schema","title":"Install a custom schema","text":"<p>If you have installed OntoGPT directly from its GitHub repository, then you may install a custom schema like this:</p> <ol> <li>Move the schema file to the <code>src/ontogpt/templates</code> directory.</li> <li>Run <code>make</code> from the root of the repository to generate Pydantic versions of the schema.</li> </ol> <p>If you have installed OntoGPT from <code>pip</code>, or if you can't use the <code>make</code> command, the process is similar, though it will depend on where the package is installed.</p> <ol> <li> <p>Use the LinkML <code>gen-pydantic</code> tool to generate Pydantic classes. If your schema is named <code>alfred.yaml</code>, then run the following:</p> <p><code>bash gen-pydantic --pydantic_version 2 alfred.yaml &gt; alfred.py</code></p> </li> <li> <p>Move both the .yaml and the .py versions of your schema to the <code>templates</code> directory of wherever OntoGPT is installed. In a virtual environment named <code>temp</code> that may be something like <code>/temp/lib/python3.9/site-packages/ontogpt/templates</code>.</p> </li> </ol> <p>You may then use the schema like any other. For example, if your schema is named <code>albatross.yaml</code>, then an extract command is:</p> <pre><code>ontogpt extract -t albatross -i input.txt\n</code></pre>"},{"location":"evaluations/","title":"Evaluations","text":"<p>OpenAI's functions have been evaluated on test data sets.</p> <p>All evaluation results include OpenAI cache databases (<code>openai_cache.db.gz</code>) as a reference of the prompts and responses obtained during the evaluation. This may be used by extracting the cache database to the root directory of the project.</p> <p>Tests may be run with the <code>eval</code> command followed by the test name, e.g., for the BC5CDR test below:</p> <pre><code>ontogpt eval EvalCTD \n</code></pre> <p>By default, the evaluation will only be executed over a subset of the test corpus.</p> <p>The exact number of inputs to run the test over can be controlled with the <code>--num-tests</code> option, like this:</p> <pre><code>ontogpt eval --num-tests 1 EvalCTD\n</code></pre> <p>To run the full set of tests, set <code>num-tests</code> to the input count for a given evaluation, as defined below.</p> <p>For each document, the evaluation process will attempt to process the full text without preprocessing.</p> <p>If the <code>--chunking</code> option is used, then the input text will instead be chunked into segments of a few tokens each, essentially creating new queries for each segment. This alternate strategy may impact test results and will result in longer run time.</p>"},{"location":"evaluations/#bc5cdr","title":"BC5CDR","text":"<p>Test Name: EvalCTD Input Count: 500</p> <p>Results for OntoGPT on the BioCreative V Chemical Disease Relation Task (BC5CDR) are available on Zenodo here: https://zenodo.org/record/7657763</p> <p>Evaluation functions for BC5CDR are available in <code>src/ontogpt/evaluation/ctd/eval_ctd.py</code>.</p> <p>Note that the project also includes test and train data (<code>CDR_TestSet.BioC.xml.gz</code> and <code>CDR_TrainSet.BioC.xml.gz</code>, respectively) as well as a set of synonyms for MeSH terms (see <code>synonyms.yaml</code>).</p>"},{"location":"evaluations/#template","title":"Template","text":"<p>This evaluation uses the following schema template:</p> <pre><code>id: http://w3id.org/ontogpt/ctd\nname: ctd\ntitle: Chemical to Disease Template\ndescription: &gt;-\n  A template for Chemical to Disease associations.\n\n  This template is intended to represent associations between chemicals and diseases,\n  and for evaluating Semantic Llama against BioCreative V Chemical Disease\n  Relation (CDR) Task (BC5CDR).\nsee_also:\n  - https://biocreative.bioinformatics.udel.edu/media/store/files/2015/BC5CDR_overview.final.pdf\n  - https://academic.oup.com/database/article/doi/10.1093/database/baw068/2630414\nsource: https://biocreative.bioinformatics.udel.edu/tasks/biocreative-v/track-3-cdr/\nlicense: https://creativecommons.org/publicdomain/zero/1.0/\nprefixes:\n  linkml: https://w3id.org/linkml/\n  drug: http://w3id.org/ontogpt/drug/\n\ndefault_prefix: drug\ndefault_range: string\n\nimports:\n  - linkml:types\n  - core\n\nclasses:\n\n  ChemicalToDiseaseDocument:\n    description: A document that contains chemical to disease relations.\n    is_a: TextWithTriples\n    slot_usage:\n      triples:\n        range: ChemicalToDiseaseRelationship\n        annotations:\n          prompt: &gt;-\n            A semi-colon separated list of chemical to disease relationships, where the relationship is either INDUCES\n            or TREATS.\n            for example: Lidocaine INDUCES cardiac asystole; \n            Hydroxychloroquine NOT TREATS COVID-19;\n            Methyldopa INDUCES Hypotension;\n            Monosodium Glutamate NOT INDUCES Headache;\n            Imatinib TREATS cancer\n          exclude: Lidocaine, cardiac asystole, Hydroxychloroquine, COVID-19, Methyldopa, Headache, Imatinib, cancer\n\n\n  ChemicalToDiseaseRelationship:\n    is_a: Triple\n    description: A triple where the subject is a chemical and the object is a disease.\n    slot_usage:\n      subject:\n        range: Chemical\n        description: &gt;-\n          The chemical substance, drug, or small molecule. \n          For example: Lidocaine, Monosodium Glutamate, Imatinib.\n      object:\n        range: Disease\n        description: &gt;-\n          The disease or condition that is being treated or induced by the chemical.\n          For example, asthma, cancer, covid-19, cardiac asystole, Hypotension, Headache.\n      predicate:\n        range: ChemicalToDiseasePredicate\n        description: The relationship type, e.g. INDUCES, TREATS.\n      subject_qualifier:\n        range: NamedEntity\n        description: &gt;-\n          An optional qualifier or modifier for the chemical, e.g. \"high dose\" or \"intravenously administered\"\n      object_qualifier:\n        range: NamedEntity\n        description: &gt;-\n          An optional qualifier or modifier for the disease, e.g. \"severe\" or \"with additional complications\"\n\n  Disease:\n    is_a: NamedEntity\n    annotations:\n      annotators: \"sqlite:obo:mesh, sqlite:obo:mondo, sqlite:obo:hp, sqlite:obo:ncit, sqlite:obo:doid, bioportal:meddra\"\n      prompt.examples: cardiac asystole, COVID-19, Headache, cancer\n    id_prefixes:\n      - MESH\n    slot_usage:\n      id:\n        pattern: \"^MESH:[CD][0-9]{6}$\"\n        values_from:\n          - MeshDiseaseIdentifier\n\n  Chemical:\n    is_a: NamedEntity\n    annotations:\n      annotators: \"sqlite:obo:mesh, sqlite:obo:chebi, sqlite:obo:ncit, bioportal:mdm, sqlite:obo:drugbank, gilda:\"\n      prompt.examples: Lidocaine, Hydroxychloroquine, Methyldopa, Imatinib\n    id_prefixes:\n      - MESH\n    slot_usage:\n      id:\n        pattern: \"^MESH:[CD][0-9]{6}$\"\n        values_from:\n          - MeshChemicalIdentifier\n\n  ChemicalToDiseasePredicate:\n    is_a: RelationshipType\n    description: &gt;-\n      A predicate for chemical to disease relationships\n    comments:\n      - for the purposes of evaluation against BC5CDR, any predicate other than INDUCES is ignored.\n\nenums:\n\n  MeshChemicalIdentifier:\n    reachable_from:\n      source_ontology: obo:mesh\n      source_nodes:\n        - MESH:D000602 ## Amino Acids, Peptides, and Proteins\n        - MESH:D001685 ## Biological Factors\n        - MESH:D002241 ## Carbohydrates\n        - MESH:D004364 ## Pharmaceutical Preparations\n        - MESH:D006571 ## Heterocyclic Compounds\n        - MESH:D007287 ## Inorganic Chemicals\n        - MESH:D008055 ## Lipids\n        - MESH:D009706 ## Nucleic Acids, Nucleotides, and Nucleosides\n        - MESH:D009930 ## Organic Chemicals\n        - MESH:D011083 ## Polycyclic Compounds\n        - MESH:D013812 ## Therapeutics\n        - MESH:D019602 ## Food and Beverages\n        - MESH:D045424 ## Complex Mixtures\n        - MESH:D045762 ## Enzymes and Coenzymes\n        - MESH:D046911 ## Macromolecular Substances\n  MeshDiseaseIdentifier:\n    reachable_from:\n      source_ontology: obo:mesh\n      source_nodes:\n        - MESH:D001423 ## Bacterial Infections and Mycoses\n        - MESH:D001523 ## Mental Disorders\n        - MESH:D002318 ## Cardiovascular Diseases\n        - MESH:D002943 ## Circulatory and Respiratory Physiological Phenomena\n        - MESH:D004066 ## Digestive System Diseases\n        - MESH:D004700 ## Endocrine System Diseases\n        - MESH:D005128 ## Eye Diseases\n        - MESH:D005261 ## Female Urogenital Diseases and Pregnancy Complications\n        - MESH:D006425 ## Hemic and Lymphatic Diseases\n        - MESH:D007154 ## Immune System Diseases\n        - MESH:D007280 ## Disorders of Environmental Origin\n        - MESH:D009057 ## Stomatognathic Diseases\n        - MESH:D009140 ## Musculoskeletal Diseases\n        - MESH:D009358 ## Congenital, Hereditary, and Neonatal Diseases and Abnormalities\n        - MESH:D009369 ## Neoplasms\n        - MESH:D009422 ## Nervous System Diseases\n        - MESH:D009750 ## Nutritional and Metabolic Diseases\n        - MESH:D009784 ## Occupational Diseases\n        - MESH:D010038 ## Otorhinolaryngologic Diseases\n        - MESH:D010272 ## Parasitic Diseases\n        - MESH:D012140 ## Respiratory Tract Diseases\n        - MESH:D013568 ## Pathological Conditions, Signs and Symptoms\n        - MESH:D014777 ## Virus Diseases\n        - MESH:D014947 ## Wounds and Injuries\n        - MESH:D017437 ## Skin and Connective Tissue Diseases\n        - MESH:D052801 ## Male Urogenital Diseases\n        - MESH:D064419 ## Chemically-Induced Disorders\n</code></pre>"},{"location":"functions/","title":"OntoGPT Functions","text":"<p>All OntoGPT functions are run from the command line.</p> <p>Precede all commands with <code>ontogpt</code>.</p> <p>To see the full list of available commands, run this:</p> <pre><code>ontogpt --help\n</code></pre>"},{"location":"functions/#basic-parameters","title":"Basic Parameters","text":"<p>To see verbose output, run:</p> <pre><code>ontogpt -v\n</code></pre> <p>The options <code>-vv</code> and <code>-vvv</code> will enable progressively more verbose output.</p>"},{"location":"functions/#cache-db","title":"cache-db","text":"<p>Use the option <code>--cache-db</code> to specify a path to a sqlite database to cache the prompt-completion results.</p>"},{"location":"functions/#skip-annotator","title":"skip-annotator","text":"<p>Use the option <code>--skip-annotator</code> to skip one or more annotators (e.g. <code>--skip-annotator gilda</code>).</p>"},{"location":"functions/#common-parameters","title":"Common Parameters","text":"<p>The following options are available for most functions unless stated otherwise.</p>"},{"location":"functions/#inputfile","title":"inputfile","text":"<p>Use the option <code>--inputfile</code> to specify a path to a file containing input text.</p>"},{"location":"functions/#template","title":"template","text":"<p>Use the option <code>--template</code> to specify a template to use. This is a required parameter.</p> <p>Only the name is required, without any filename suffix.</p> <p>To use the <code>gocam</code> template, for example, the parameter will be <code>--template gocam</code></p> <p>This may be one of the templates included with OntoGPT or a custom template, but in the latter case, the schema, generated Pydantic classes, and any imported schemas should be present in the same location.</p>"},{"location":"functions/#target-class","title":"target-class","text":"<p>Use the option <code>--target-class</code> to specify a class in a schema to treat as the root.</p> <p>If a schema does not already specify a root class, this is required.</p> <p>Alternatively, the target class can be specified as part of the <code>--template</code> option, like so: <code>--template mendelian_disease.MendelianDisease</code></p>"},{"location":"functions/#model","title":"model","text":"<p>Use the option <code>model</code> to specify the name of a large language model to be used.</p> <p>For example, this may be <code>--model gpt-4</code>.</p> <p>Consult the full list of available models with:</p> <pre><code>ontogpt list-models\n</code></pre>"},{"location":"functions/#recurse","title":"recurse","text":"<p>Use the option <code>recurse</code> to specify whether recursion should be used when parsing the schema.</p> <p>Recursion is on by default.</p> <p>Disable it with <code>--no-recurse</code>.</p>"},{"location":"functions/#use-textract","title":"use-textract","text":"<p>Use the option <code>use-textract</code> to specify whether to use the textract package to extract text from the input document. Textract supports retrieving raw text from PDFs, images, audio, and a variety of other formats.</p> <p>Textract extraction is off by default.</p> <p>Enable it with <code>--use-textract</code>.</p>"},{"location":"functions/#output","title":"output","text":"<p>Use the option <code>output</code> to provide a path to write an output file to.</p> <p>If this path is not provided, OntoGPT will write to stdout.</p>"},{"location":"functions/#output-format","title":"output-format","text":"<p>Use the option <code>output-format</code> to specify the desired output file format.</p> <p>This may be one of:</p> <ul> <li>html</li> <li>json</li> <li>jsonl</li> <li>md</li> <li>owl</li> <li>pickle</li> <li>turtle</li> <li>yaml</li> </ul>"},{"location":"functions/#auto-prefix","title":"auto-prefix","text":"<p>Use the option <code>auto-prefix</code> to define a prefix to use for entities without a matching namespace.</p> <p>When OntoGPT's extract functions find an entity matching the input schema but cannot ground it, the entity will still be included in the output.</p> <p>By default, these entities will be assigned identifiers like <code>AUTO:tangerine</code>. If you ground this term to the Food Ontology, however, the entity may be <code>FOODON:00003488</code> instead.</p>"},{"location":"functions/#show-prompt","title":"show-prompt","text":"<p>Use the option <code>show-prompt</code> to show all prompts constructed and sent to the model. Otherwise, only the final prompt will be shown.</p> <p>Showing the full prompt is off by default.</p> <p>Enable it by using <code>--show-prompt</code> and setting the verbosity level to high (<code>-vvv</code>).</p>"},{"location":"functions/#functions","title":"Functions","text":""},{"location":"functions/#categorize-mappings","title":"categorize-mappings","text":"<p>Categorize a collection of mappings in the Simple Standard for Sharing Ontological Mappings (SSSOM) format.</p> <p>Mappings in this format may not include their specific mapping types (e.g., broad or close mappings).</p> <p>This function will attempt to apply more specific mappings wherever possible.</p> <p>Example:</p> <p>Using an example SSSOM mapping collection</p> <pre><code>ontogpt categorize-mappings -i mp-hp-exact-0.0.1.sssom.tsv\n</code></pre> <p>Note that OntoGPT will attempt to retrieve the resources specified in the mapping (in the above example, that will include HP and MP). If it cannot find a corresponding resource it will raise a HTTP 404 error.</p>"},{"location":"functions/#clinical-notes","title":"clinical-notes","text":"<p>Create mock clinical notes.</p> <p>Options:</p> <ul> <li><code>-d</code>, <code>--description TEXT</code> - a text description of the contents of the generated notes.</li> <li><code>--sections TEXT</code> - sections to include in the generated notes, for example, medications, vital signs. Use multiple times for multiple sections, e.g., <code>--sections medications --sections \"vital signs\"</code></li> </ul> <p>Example:</p> <pre><code>ontogpt clinical-notes -d \"middle-aged female patient with syncope and recent travel to the Amazon rainforest\"\n</code></pre>"},{"location":"functions/#complete","title":"complete","text":"<p>Prompt completion.</p> <p>Given the path to a file containing text to continue, this command will simply pass it to the model as a completion task.</p> <p>Example:</p> <p>The file <code>example2.txt</code> contains the text \"Here's a good joke about high blood pressure:\"</p> <pre><code>ontogpt complete example2.txt\n</code></pre> <p>We take no responsibility for joke quality or lack thereof.</p>"},{"location":"functions/#convert","title":"convert","text":"<p>Convert output format.</p> <p>Rather than a direct format translation, this function performs a full SPIRES extraction on the input file and writes the output in the specified format.</p> <p>Example:</p> <pre><code>ontogpt convert -o outputfile.md -O md inputfile.yaml\n</code></pre>"},{"location":"functions/#convert-examples","title":"convert-examples","text":"<p>Convert training examples from YAML.</p> <p>This can be necessary for performing evaluations.</p> <p>Given the path to a YAML-format input file containing training examples in a format like this:</p> <pre><code>---\nexamples:\n    - prompt: &lt;text prompt&gt;\n      completion: &lt;text of completion of the prompt&gt;\n    - prompt: &lt;another text prompt&gt;\n      completion: &lt;text of completion of another prompt&gt;\n</code></pre> <p>the function will convert it to equivalent JSON.</p> <p>Example:</p> <pre><code>ontogpt convert-examples inputfile.yaml\n</code></pre>"},{"location":"functions/#convert-geneset","title":"convert-geneset","text":"<p>Convert gene set to YAML.</p> <p>The gene set may be in JSON (msigdb format) or text (one gene symbol per line) format.</p> <p>See also the <code>create-gene-set</code> command (see below).</p> <p>Options:</p> <ul> <li><code>--fill</code> / <code>--no-fill</code> - Defaults to False (<code>--no-fill</code>). If True (<code>--fill</code>), the function will attempt to fill in missing gene values.</li> <li><code>-U</code>, <code>--input-file TEXT</code> - Path to a file with gene IDs to enrich (if not passed as arguments).</li> </ul> <p>Example:</p> <pre><code>ontogpt convert-geneset -U inputfile.json\n</code></pre>"},{"location":"functions/#create-gene-set","title":"create-gene-set","text":"<p>Create a gene set.</p> <p>This is primarily relevant to the TALISMAN method for creating gene set summaries.</p> <p>It creates a gene set given a set of gene annotations in two-column TSV or GAF format.</p> <p>The function also requires a single argument for the term to create the gene set with.</p> <p>The output is provided in YAML format.</p> <p>Options:</p> <ul> <li><code>-A</code>, <code>--annotation-path TEXT</code> - Path to a file containing annotations.</li> </ul> <p>Example:</p> <pre><code>ontogpt create-gene-set -A inputfile.tsv \"positive regulation of mitotic cytokinesis\"\n</code></pre>"},{"location":"functions/#diagnose","title":"diagnose","text":"<p>Diagnose a clinical case represented as one or more Phenopackets.</p> <p>This function takes one or more file paths as arguments, where each must contain a phenopacket in JSON format.</p> <p>Example inputs may be found at the Phenomics Exchange repository.</p> <p>Example:</p> <pre><code>ontogpt diagnose case1.json case2.json \n</code></pre>"},{"location":"functions/#dump-completions","title":"dump-completions","text":"<p>Dump cached completions.</p> <p>OntoGPT saves queries and successful text completions to an sqlite database.</p> <p>Caching is not currently supported for local models.</p> <p>Use this function to retrieve the contents of this database.</p> <p>See also: the <code>cache-db</code> parameter described above.</p> <p>Options:</p> <ul> <li><code>-m TEXT</code> - Match string to use for filtering.</li> <li><code>-D TEXT</code> - Path to sqlite database.</li> </ul> <p>Example:</p> <pre><code>ontogpt dump-completions -m \"soup\"\n</code></pre>"},{"location":"functions/#embed","title":"embed","text":"<p>Embed text.</p> <p>This function will return an embedding vector for the input text or texts.</p> <p>Embedding retrieval is not currently supported for local models.</p> <p>Options:</p> <ul> <li><code>-C</code>, <code>--context TEXT</code> - domain e.g. anatomy, industry, health-related</li> </ul> <p>Example:</p> <pre><code>ontogpt embed \"obstreperous muskrat\"\n</code></pre> <p>For OpenAI's \"text-embedding-ada-002\" model, the output will be a vector of length 1536, like so:</p> <pre><code>[-0.015013165771961212, -0.013102399185299873, -0.005333086010068655, ...]\n</code></pre>"},{"location":"functions/#enrichment","title":"enrichment","text":"<p>Gene class summary enriching. This is OntoGPT's implementation of TALISMAN.</p> <p>The goal of gene summary enrichment is to assemble a textual summary of the functions of a set of genes and their products.</p> <p>TALISMAN can run in three different ways:</p> <ol> <li>Map gene symbols to IDs using the resolver (unless IDs are specified)</li> <li>Fetch gene descriptions using Alliance API</li> <li>Create a prompt using descriptions</li> </ol> <p>Options:</p> <ul> <li><code>-r</code>, <code>--resolver TEXT</code> - OAK selector for the gene ID resolver, e.g., <code>sqlite:obo:hgnc</code> for HGNC gene IDs.</li> <li><code>-C</code>, <code>--context TEXT</code> - domain, e.g., anatomy, industry, health-related</li> <li><code>--strict</code> / <code>--no-strict</code> - If set, there must be a unique mappings from labels to IDs. Defaults to True.</li> <li><code>-U</code>, <code>--input-file TEXT</code> - Path to a file with gene IDs to enrich if not passed as arguments.</li> <li><code>--randomize-gene-descriptions-using-file TEXT</code> - For evaluation only. Path to a file containing gene identifiers and descriptions; if this option is used, TALISMAN will swap out gene descriptions with those from this gene set file.</li> <li><code>--ontological-synopsis</code> / <code>--no-ontological-synopsis</code> - If set, use automated rather than manual gene descriptions. Defaults to True.</li> <li><code>--combined-synopsis</code> / <code>--no-combined-synopsis</code> - If set, combine gene descriptions. Defaults to False.</li> <li><code>--end-marker TEXT</code> - Specify a character or string to end prompts with. For testing minor variants of prompts.</li> <li><code>--annotations</code> / <code>--no-annotations</code> - If set, include annotations in the prompt. Defaults to True.</li> <li><code>--prompt-template TEXT</code> - Path to a file containing the prompt.</li> <li><code>--interactive</code> / <code>--no-interactive</code> - Interactive mode - rather than call the API, the function will present a walkthrough process. Defaults to False.</li> </ul> <p>Example:</p> <pre><code>ontogpt enrichment -r sqlite:obo:hgnc -U tests/input/genesets/EDS.yaml\n</code></pre> <p>In this case, the prompt will include gene summaries retrieved from the database.</p> <p>The response text will include, among other fields, a summary like this:</p> <pre><code>Summary: The common function among these genes is their involvement in the regulation and organization of the extracellular matrix, particularly collagen fibril organization and biosynthesis.\n</code></pre>"},{"location":"functions/#entity-similarity","title":"entity-similarity","text":"<p>Determine similarity between ontology entities by comparing their embeddings.</p> <p>Options:</p> <ul> <li><code>-r</code>, <code>--ontology TEXT</code> - name of the ontology to use. This should be an OAK adapter name such as \"sqlite:obo:hp\".</li> <li><code>--definitions</code> / <code>--no-definitions</code> - Include text definitions in the text to embed. Defaults to True.</li> <li><code>--parents</code> / <code>--no-parents</code> - Include is-a parent terms in the text to embed. Defaults to True.</li> <li><code>--ancestors</code> / <code>--no-ancestors</code> - Include all ancestors in the text to embed. Defaults to True.</li> <li><code>--logical-definitions</code> / <code>--no-logical-definitions</code>- Include logical definitions in the text to embed. Defaults to True.</li> <li><code>--autolabel</code> / <code>--no-autolabel</code> - Add labels to each subject and object identifier. Defaults to True.</li> <li><code>--synonyms</code> / <code>--no-synonyms</code> - Include synonyms in the text to embed. Defaults to True.</li> </ul> <p>Example:</p> <pre><code>ontogpt entity-similarity -r sqlite:obo:hp HP:0012228 HP:0000629\n</code></pre> <p>In this case, the output will look like this:</p> <pre><code>subject_id      subject_label   object_id       object_label    embedding_cosine_similarity     object_rank_for_subject\nHP:0012228      Tension-type headache   HP:0012228      Tension-type headache   0.9999999999999999      0\nHP:0012228      Tension-type headache   HP:0000629      Periorbital fullness    0.7755551231762359      1\nHP:0000629      Periorbital fullness    HP:0000629      Periorbital fullness    1.0000000000000002      0\nHP:0000629      Periorbital fullness    HP:0012228      Tension-type headache   0.7755551231762359      1\n</code></pre>"},{"location":"functions/#eval","title":"eval","text":"<p>Evaluate an extractor.</p> <p>See the Evaluations section for more details.</p> <p>Options:</p> <ul> <li><code>--num-tests INTEGER</code> - number of test iterations to cycle through. Defaults to 5.</li> <li><code>--chunking</code> / <code>--no-chunking</code> - If set, chunk input text, then prepare a separate prompt for each chunk. Otherwise the full input text is passed. Defaults to False.</li> </ul> <p>Example:</p> <pre><code>ontogpt eval --num-tests 1 EvalCTD\n</code></pre>"},{"location":"functions/#eval-enrichment","title":"eval-enrichment","text":"<p>Run enrichment (TALISMAN) using multiple methods.</p> <p>This function runs a set of evaluations specific to the TALISMAN gene set summary process.</p> <p>It will iterate through all relevant models to compare results.</p> <p>The function assumes genes will have HGNC identifiers.</p> <p>Options:</p> <ul> <li><code>--strict</code> / <code>--no-strict</code> - If set, there must be a unique mappings from labels to IDs. Defaults to True.</li> <li><code>-U</code>, <code>--input-file TEXT</code> - Path to a file with gene IDs to enrich (if not passed as arguments)</li> <li><code>--ontological-synopsis</code> / <code>--no-ontological-synopsis</code> - If set, use automated rather than manual gene descriptions. Defaults to True.</li> <li><code>--combined-synopsis</code> / <code>--no-combined-synopsis</code> - If set, combine gene descriptions. Defaults to False.</li> <li><code>--annotations</code> / <code>--no-annotations</code> - If set, include annotations in the prompt. Defaults to True.</li> <li><code>-n</code>, <code>--number-to-drop INTEGER</code> - Maximum number of genes to drop if necessary.</li> <li><code>-A</code>, <code>--annotations-path TEXT</code> - Path to file containing annotations.</li> </ul> <p>Example:</p> <pre><code>ontogpt enrichment -U tests/input/genesets/EDS.yaml\n</code></pre>"},{"location":"functions/#extract","title":"extract","text":"<p>Extract knowledge from text guided by a schema.</p> <p>This is OntoGPT's implementation of SPIRES.</p> <p>Output includes the input text (or a truncated part), the raw completion output, the prompt (specifically, the last iteration of the prompts used), and an extracted object containing all parts identified in the input text, as well as a list of named entities and their labels.</p> <p>Options:</p> <ul> <li><code>-S</code>, <code>--set-slot-value TEXT</code> - Set slot value manually, e.g., <code>--set-slot-value has_participant=protein</code></li> </ul> <p>Examples:</p> <pre><code>ontogpt extract -t gocam.GoCamAnnotations -i tests/input/cases/gocam-33246504.txt\n</code></pre> <p>In this case, you will an extracted object in the output like:</p> <pre><code>extracted_object:\n  genes:\n    - HGNC:5992\n    - AUTO:F4/80\n    - HGNC:16400\n    - HGNC:1499\n    - HGNC:5992\n    - HGNC:5993\n  organisms:\n    - NCBITaxon:10088\n    - AUTO:bone%20marrow-derived%20macrophages\n    - AUTO:astrocytes\n    - AUTO:bipolar%20cells\n    - AUTO:vascular%20cells\n    - AUTO:perivascular%20MPs\n  gene_organisms:\n    - gene: HGNC:5992\n      organism: AUTO:mononuclear%20phagocytes\n    - gene: HGNC:16400\n      organism: AUTO:F4/80%2B%20mononuclear%20phagocytes\n    - gene: HGNC:1499\n      organism: AUTO:F4/80%2B%20mononuclear%20phagocytes\n    - gene: HGNC:5992\n      organism: AUTO:perivascular%20macrophages\n    - gene: HGNC:5993\n      organism: AUTO:None\n  activities:\n    - GO:0006954\n    - AUTO:photoreceptor%20death\n    - AUTO:retinal%20function\n  gene_functions:\n    - gene: HGNC:5992\n      molecular_activity: GO:0006954\n    - gene: AUTO:F4/80\n      molecular_activity: AUTO:mononuclear%20phagocyte%20recruitment\n    - gene: HGNC:1499\n      molecular_activity: GO:0006954\n    - gene: HGNC:5992\n      molecular_activity: AUTO:immune-specific%20expression\n    - gene: HGNC:5993\n      molecular_activity: AUTO:IL-1%CE%B2%20receptor\n    - gene: AUTO:rytvela\n      molecular_activity: AUTO:IL-1R%20modulation\n    - gene: AUTO:Kineret\n      molecular_activity: AUTO:IL-1R%20antagonism\n  cellular_processes:\n    - AUTO:macrophage-induced%20photoreceptor%20death\n  gene_localizations:\n    - gene: HGNC:5992\n      location: AUTO:subretinal%20space\n</code></pre> <p>Or, we can extract information about a drug and specify which model to use:</p> <pre><code>ontogpt extract -t drug -i tests/input/cases/drug-DB00316-moa.txt --auto-prefix UNKNOWN -m gpt-4\n</code></pre> <p>The <code>ontology_class</code> schema may be used to perform more domain-agnostic entity recognition, though this is generally incompatible with grounding.</p> <pre><code>ontogpt extract -t ontology_class -i tests/input/cases/human_urban_green_space.txt\n</code></pre>"},{"location":"functions/#fill","title":"fill","text":"<p>Fill in missing values.</p> <p>Requires the path to a file containing a data object to be passed (as an argument) and a set of examples as an input file.</p> <p>Options:</p> <ul> <li><code>-E</code>, <code>--examples FILENAME</code> - Path to a file of example objects.</li> </ul>"},{"location":"functions/#generate-extract","title":"generate-extract","text":"<p>Generate text and then extract knowledge from it.</p> <p>This command runs two operations:</p> <ol> <li>Generate a natural language description of something</li> <li>Parse the generated description using SPIRES</li> </ol> <p>For example, given a cell type such as Acinar Cell Of Salivary Gland, generate a description using GPT describing many aspects of the cell type, from its marker genes through to its function and diseases it is implicated in.</p> <p>After that, use the cell-type schema to extract this into structured form.</p> <p>As an optional next step use linkml-owl to generate OWL TBox axioms.</p> <p>See also: <code>iteratively-generate-extract</code> below.</p> <p>Example:</p> <pre><code>ontogpt generate-extract -t cell_type CL:0002623\n</code></pre>"},{"location":"functions/#iteratively-generate-extract","title":"iteratively-generate-extract","text":"<p>Iterate through generate-extract.</p> <p>This runs the <code>generate-extract</code> command in iterative mode. It will traverse the extracted subtypes with each iteration, gradually building up an ontology that is entirely generated from the \"latent knowledge\" in the LLM.</p> <p>Currently each iteration is independent so the method remains unaware as to whether it has already made a concept. Ungrounded concepts may indicate gaps in available knowledgebases.</p> <p>Unlike the <code>generate-extract</code> command, this command requires some additional parameters to be specified.</p> <p>Please specify the input ontology and the output path.</p> <p>Options:</p> <ul> <li><code>-r</code>, <code>--ontology TEXT</code> - Ontology to use. Use the OAK selector format, e.g., \"sqlite:obo:cl\"</li> <li><code>-M</code>, <code>--max-iterations INTEGER</code> - Maximum number of iterations.</li> <li><code>-I</code>, <code>--iteration-slot TEXT</code> - Slots to iterate over.</li> <li><code>-D</code>, <code>--db TEXT</code> - Path to the output, in YAML format.</li> <li><code>--clear</code> / <code>--no-clear</code> - If set, clear the output database before starting. Defaults to False.</li> </ul> <p>Example:</p> <pre><code>ontogpt iteratively-generate-extract -t cell_type -r sqlite:obo:cl -D cells.yaml CL:0002623 \n</code></pre>"},{"location":"functions/#list-models","title":"list-models","text":"<p>List all available models.</p> <p>Example:</p> <pre><code>ontogpt list-models\n</code></pre>"},{"location":"functions/#list-templates","title":"list-templates","text":"<p>List the templates.</p> <p>Alternatively, run <code>make list_templates</code>.</p> <p>Example:</p> <pre><code>ontogpt list-templates\n</code></pre>"},{"location":"functions/#pubmed-annotate","title":"pubmed-annotate","text":"<p>Retrieve a collection of PubMed IDs for a given search, then perform extraction on them with SPIRES.</p> <p>The search argument will accept all parameters known to PubMed search, such as filtering by publication year.</p> <p>Works for single publications, too - set the <code>--limit</code> parameter to 1 and specify a PubMed ID as the search argument.</p> <p>Options:</p> <ul> <li><code>--limit INTEGER</code> - Total number of citation records to return. Limited by the NCBI API.</li> <li><code>--get-pmc</code> / <code>--no-get-pmc</code> - Attempt to parse PubMed Central full text(s) rather than abstract(s) alone.</li> </ul> <p>Examples:</p> <pre><code>ontogpt pubmed-annotate -t phenotype \"Takotsubo Cardiomyopathy: A Brief Review\" --get-pmc --model gpt-3.5-turbo-16k --limit 3\n</code></pre> <pre><code>ontogpt pubmed-annotate -t environmental_sample \"33126925\" --limit 1\n</code></pre> <pre><code>ontogpt pubmed-annotate -t composite_disease \"(earplugs) AND ((\"1950\"[Date - Publication] : \"1990\"[Date - Publication]))\" --limit 4\n</code></pre>"},{"location":"functions/#pubmed-extract","title":"pubmed-extract","text":"<p>Extract knowledge from a single PubMed ID.</p> <p>DEPRECATED - use <code>pubmed-annotate</code> instead.</p>"},{"location":"functions/#recipe-extract","title":"recipe-extract","text":"<p>Extract from a recipe on the web.</p> <p>This uses the <code>recipe</code> template and the recipe_scrapers package. The latter supports many different recipe web sites, so give your favorite a try.</p> <p>Pass a URL as the argument, or use the -R option to specify the path to a file containing one URL per line.</p> <p>Options:</p> <ul> <li><code>-R</code>, <code>--recipes-urls-file TEXT</code> - File with URLs to recipes to use for extraction.</li> </ul> <p>Example:</p> <pre><code>ontogpt recipe-extract https://www.allrecipes.com/recipe/17445/grilled-asparagus/\n</code></pre> <p>In this case, expect an extracted object like the following:</p> <pre><code>extracted_object:\n  url: https://www.allrecipes.com/recipe/17445/grilled-asparagus/\n  label: Grilled Asparagus\n  description: Grilled asparagus with olive oil, salt, and pepper.\n  categories:\n    - AUTO:None\n  ingredients:\n    - food_item:\n        food: FOODON:03311349\n        state: fresh, spears\n      amount:\n        value: '1'\n        unit: UO:0010034\n    - food_item:\n        food: FOODON:03301826\n      amount:\n        value: '1'\n        unit: UO:0010042\n    - food_item:\n        food: AUTO:salt\n        state: and pepper\n      amount:\n        value: N/A\n        unit: AUTO:N/A\n  steps:\n    - action: AUTO:Preheat\n      inputs:\n        - food: AUTO:outdoor%20grill\n          state: None\n      outputs:\n        - food: AUTO:None\n          state: None\n      utensils:\n        - AUTO:None\n    - action: dbpediaont:season\n      inputs:\n        - food: FOODON:00003458\n          state: coated\n        - food: AUTO:salt\n          state: None\n        - food: FOODON:00003520\n      outputs:\n        - food: FOODON:00003458\n          state: seasoned\n      utensils:\n        - AUTO:None\n    - action: AUTO:cook\n      inputs:\n        - food: FOODON:03311349\n          state: None\n      outputs:\n        - food: FOODON:03311349\n          state: cooked\n      utensils:\n        - AUTO:grill\n</code></pre>"},{"location":"functions/#synonyms","title":"synonyms","text":"<p>Extract synonyms, based on embeddings.</p> <p>The context parameter is required.</p> <p>Options:</p> <ul> <li><code>-C</code>, <code>--context TEXT</code> - domain, e.g., anatomy, industry, health-related</li> </ul> <p>Example:</p> <pre><code>ontogpt synonyms --context astronomy star\n</code></pre>"},{"location":"functions/#text-distance","title":"text-distance","text":"<p>Embed text and calculate euclidian distance between the embeddings.</p> <p>The terms must be separated by an <code>@</code> character.</p> <p>Options:</p> <ul> <li><code>-C</code>, <code>--context TEXT</code> - domain, e.g., anatomy, industry, health-related</li> </ul> <p>Example:</p> <pre><code>ontogpt text-distance pancakes @ syrup\n</code></pre>"},{"location":"functions/#text-similarity","title":"text-similarity","text":"<p>Like <code>text-distance</code>, this command compares the embeddings of input terms.</p> <p>This command returns the cosine similarity of the embedding vectors.</p> <p>Options:</p> <ul> <li><code>-C</code>, <code>--context TEXT</code> - domain, e.g., anatomy, industry, health-related</li> </ul> <p>Example:</p> <pre><code>ontogpt text-similarity basketball @ basket-weaving\n</code></pre>"},{"location":"functions/#web-extract","title":"web-extract","text":"<p>Extract knowledge from web page.</p> <p>Pass a URL as an argument and OntoGPT will use the SPIRES method to extract information based on the specified template.</p> <p>Because this depends upon scraping a page, results may vary depending on a site's complexity and structure.</p> <p>Even relatively short pages may exceed a model's context size, so larger context models may be necessary.</p> <p>Example:</p> <pre><code>ontogpt web-extract -t reaction.Reaction -m gpt-3.5-turbo-16k https://www.scienceofcooking.com/maillard_reaction.htm \n</code></pre>"},{"location":"functions/#wikipedia-extract","title":"wikipedia-extract","text":"<p>Extract knowledge from a Wikipedia page.</p> <p>Pass an article title as an argument and OntoGPT will use the SPIRES method to extract information based on the specified template.</p> <p>Even relatively short pages may exceed a model's context size, so larger context models may be necessary.</p> <p>Example:</p> <pre><code>ontogpt wikipedia-extract -t mendelian_disease.MendelianDisease -m gpt-3.5-turbo-16k \"Cartilage\u2013hair hypoplasia\"\n</code></pre>"},{"location":"functions/#wikipedia-search","title":"wikipedia-search","text":"<p>Extract knowledge from Wikipedia pages based on a search.</p> <p>Pass a search phrase as an argument and OntoGPT will use the SPIRES method to extract information based on the specified template.</p> <p>Even relatively short pages may exceed a model's context size, so larger context models may be necessary.</p> <p>Example:</p> <pre><code>ontogpt wikipedia-search -t biological_process -m gpt-3.5-turbo-16k \"digestion\"\n</code></pre>"},{"location":"operation/","title":"Operation","text":""},{"location":"operation/#getting-started","title":"Getting Started","text":"<p>OntoGPT is run from the command line. See the full list of commands with:</p> <pre><code>ontogpt --help\n</code></pre> <p>For a simple example of text completion and testing to ensure OntoGPT is set up correctly, create a text file containing the following, saving the file as <code>example.txt</code>:</p> <pre><code>Why did the squid cross the coral reef?\n</code></pre> <p>Then try the following command:</p> <pre><code>ontogpt complete example.txt\n</code></pre> <p>You should get text output like the following:</p> <pre><code>Perhaps the squid crossed the coral reef for a variety of reasons:\n\n1. Food: Squids are known to feed on small fish and other marine organisms, and there could have been a rich food source on the other side of the reef.\n\n...\n</code></pre> <p>OntoGPT is intended to be used for information extraction. The following examples show how to accomplish this.</p>"},{"location":"operation/#strategy-1-knowledge-extraction-using-spires","title":"Strategy 1: Knowledge extraction using SPIRES","text":""},{"location":"operation/#working-mechanism","title":"Working Mechanism","text":"<ol> <li>You provide an arbitrary data model, describing the structure you want to extract text into. This can be nested (but see limitations below). The predefined templates may be used.</li> <li>Provide your preferred annotations for grounding <code>NamedEntity</code> fields</li> <li>OntoGPT will:<ul> <li>Generate a prompt</li> <li>Feed the prompt to a language model</li> <li>Parse the results into a dictionary structure</li> <li>Ground the results using a preferred annotator (e.g., an ontology)</li> </ul> </li> </ol>"},{"location":"operation/#input","title":"Input","text":"<p>Consider some text from one of the input files being used in the OntoGPT test suite. You can find the text file here. You can download the raw file from the GitHub link to that input text file, or copy its contents over into another file, say, <code>abstract.txt</code>. An excerpt:</p> <p>The cGAS/STING-mediated DNA-sensing signaling pathway is crucial   for interferon (IFN) production and host antiviral   responses</p> <p>... [snip] ...</p> <p>The underlying mechanism was the   interaction of US3 with \u03b2-catenin and its hyperphosphorylation of   \u03b2-catenin at Thr556 to block its nuclear translocation ... ...</p> <p>We can extract knowledge from the above text this into the GO pathway datamodel by running the following command:</p>"},{"location":"operation/#command","title":"Command","text":"<pre><code>ontogpt extract -t gocam.GoCamAnnotations -i ~/path/to/abstract.txt\n</code></pre> <p>Note: The value accepted by the <code>-t</code> / <code>--template</code> argument is the base name of one of the LinkML schema / data model which can be found in the templates folder.</p>"},{"location":"operation/#output","title":"Output","text":"<p>The output returned from the above command can be optionally redirected into an output file using the <code>-o</code> / <code>--output</code>.</p> <p>The following is a small part of what the larger schema-compliant output looks like:</p> <pre><code>genes:\n- HGNC:2514\n- HGNC:21367\n- HGNC:27962\n- US3\n- FPLX:Interferon\n- ISG\ngene_gene_interactions:\n- gene1: US3\n  gene2: HGNC:2514\ngene_localizations:\n- gene: HGNC:2514\n  location: Nuclear\ngene_functions:\n- gene: HGNC:2514\n  molecular_activity: Transcription\n- gene: HGNC:21367\n  molecular_activity: Production\n...\n</code></pre>"},{"location":"operation/#local-models","title":"Local Models","text":"<p>To use a local model, specify it with the <code>-m</code> or <code>--model</code> option.</p> <p>Example:</p> <pre><code>ontogpt extract -t drug -i ~/path/to/abstract.txt -m nous-hermes-13b\n</code></pre> <p>See the list of all available models with this command:</p> <pre><code>ontogpt list-models\n</code></pre> <p>When specifying a local model for the first time, it will be downloaded to your local system.</p>"},{"location":"operation/#strategy-2-gene-enrichment-using-spindoctor","title":"Strategy 2: Gene Enrichment using SPINDOCTOR","text":"<p>Given a set of genes, OntoGPT can find similarities among them.</p> <p>Ex.:</p> <pre><code>ontogpt enrichment -U tests/input/genesets/sensory-ataxia.yaml\n</code></pre> <p>The default is to use ontological gene function synopses (via the Alliance API).</p> <ul> <li>To use narrative/RefSeq summaries, use the <code>--no-ontological-synopses</code> flag</li> <li>To run without any gene descriptions, use the <code>--no-annotations</code> flag</li> </ul> <p>This strategy does not currently support using local models.</p>"},{"location":"owl_exports/","title":"OWL Exports","text":"<p>The <code>extract</code> command will let you export the results as OWL axioms, utilizing linkml-owl mappings in the schema.</p> <p>Ex.:</p> <pre><code>ontogpt extract -t recipe -i recipe-spaghetti.txt -o recipe-spaghetti.owl -O owl\n</code></pre> <p>src/ontogpt/templates/recipe.yaml is an example schema that uses linkml-owl mappings.</p> <p>See the Makefile for a full pipeline that involves using robot to extract a subset of FOODON and merge in the extracted results. This uses recipe-scrapers.</p> <p>OWL output: recipe-all-merged.owl</p> <p>Classification:</p> <p></p>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#prerequisites","title":"Prerequisites","text":"<p>OntoGPT may be installed through <code>pip</code> or used directly from the GitHub repository. In the latter case, you will need to install the <code>poetry</code> dependency manager and precede commands with <code>poetry run</code>. See the poetry installation documentation for more details.</p>"},{"location":"setup/#additional-requirements-and-options","title":"Additional requirements and options","text":"<ul> <li> <p>Python 3.9+</p> </li> <li> <p>OpenAI API key: necessary for using OpenAI's GPT models. This is a paid API and you will be charged based on usage. If you do not have an OpenAI account, you may sign up here.</p> </li> </ul> <p>You may also set additional API keys for optional resources:</p> <ul> <li>BioPortal account (for grounding). The BioPortal key is necessary for using ontologies from BioPortal. You may get a key by signing up for an account on their web site.</li> <li>NCBI E-utilities. The NCBI email address and API key are used for retrieving text and metadata from PubMed. You may still access these resources without identifying yourself, but you may encounter rate limiting and errors.</li> <li>HuggingFace Hub. This API key is necessary to retrieve models from the HuggingFace Hub service.</li> </ul>"},{"location":"setup/#installation","title":"Installation","text":"<p>To simply start using the package in your workspace:</p> <pre><code>pip install ontogpt\n</code></pre> <p>Note that some features require installing additional, optional dependencies. These may be installed as:</p> <pre><code>pip install ontogpt[extra_name]\n</code></pre> <p>where <code>extra_name</code> is one of the following:</p> <ul> <li><code>docs</code> - dependencies for building documentation</li> <li><code>web</code> - dependencies for the web application</li> <li><code>recipes</code> - dependencies for recipe scraping and parsing</li> <li><code>textract</code> - the textract plugin</li> <li><code>huggingface</code> - dependencies for accessing LLMs from HuggingFace Hub, remotely or locally</li> </ul> <p>For installation from the GitHub repository, particularly if you plan to contribute to feature development or other package code:</p> <pre><code>git clone https://github.com/monarch-initiative/ontogpt.git\ncd ontogpt/\npoetry install\n</code></pre> <p>Extras listed above may be installed as:</p> <pre><code>poetry install --extras extra_name\n</code></pre> <p>All commands should then be preceded by <code>poetry run</code>, or simply run <code>poetry shell</code> to create and enter a virtual environment for the project.</p>"},{"location":"setup/#setting-api-keys","title":"Setting API keys","text":"<p>One OntoGPT and all of its dependencies are installed, you will need to set your API keys using the Ontology Access Kit:</p> <pre><code>runoak set-apikey -e openai &lt;your openai api key&gt;\n</code></pre> <p>The optional keys may be set as follows:</p> <pre><code>runoak set-apikey -e bioportal &lt;your bioportal api key&gt;\nrunoak set-apikey -e ncbi-email &lt;your email address&gt;\nrunoak set-apikey -e ncbi-key &lt;your NCBI api key&gt;\nrunoak set-apikey -e hfhub-key &lt;your HuggingFace Hub api key&gt;\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Encountering an error, mystery, or other unexplained oddity? Open a ticket on the GitHub repository: https://github.com/monarch-initiative/ontogpt/issues</p>"},{"location":"troubleshooting/#i-get-an-error-like-typeerror-configuredbasemodel__init_subclass__-takes-no-keyword-arguments","title":"I get an error like <code>TypeError: ConfiguredBaseModel.__init_subclass__() takes no keyword arguments</code>","text":"<p>This can happen if you have installed a version of the Pydantic package older than version 2 and/or if you're using a schema generated for Pydantic v1. Versions of OntoGPT of v0.3.3 and above should prevent this from happening, but if you're still seeing the error, then running <code>make</code> again may fix it.</p>"}]}