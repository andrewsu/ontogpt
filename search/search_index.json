{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OntoGPT","text":""},{"location":"#introduction","title":"Introduction","text":"<p>OntoGPT is a Python package for the generation of Ontologies and Knowledge Bases using GPT. It is a knowledge extraction tool that uses a Large Language Models (LLMs) to extract semantic information from text.</p> <p>This makes use of so-called instruction prompts in Large Language Models (LLMs) such as GPT-4.</p> <p>Currently three different strategies for knowledge extraction have been implemented in the ontogpt package:</p> <ul> <li>SPIRES: Structured Prompt Interrogation and Recursive Extraction of Semantics</li> <li>Zero-shot learning (ZSL) approach to extracting nested semantic structures from text</li> <li>This approach takes two inputs - 1) LinkML schema 2) free text, and outputs knowledge in a structure conformant with the supplied schema in JSON, YAML, RDF or OWL formats</li> <li>Uses text-davinci-003 or gpt-3.5-turbo (gpt-4 untested)</li> <li>HALO: HAllucinating Latent Ontologies</li> <li>Few-shot learning approach to generating/hallucinating a domain ontology given a few examples</li> <li>Uses code-davinci-002</li> <li>SPINDOCTOR: Structured Prompt Interpolation of Narrative Descriptions Or Controlled Terms for Ontological Reporting</li> <li>Summarize gene set descriptions (pseudo gene-set enrichment)</li> <li>Uses text-davinci-003 or gpt-3.5-turbo (gpt-4 untested)</li> </ul>"},{"location":"#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Python 3.9+</li> <li>OpenAI account</li> </ul> <p>An OpenAI key is necessary for using OpenAI's GPT models. This is a paid API and you will be charged based on usage. If you do not have an OpenAI account, you may sign up here. You will need to set your API key using the Ontology Access Kit:</p> <pre><code>poetry run runoak set-apikey -e openai &lt;your openai api key&gt;\n</code></pre> <p>You may also set additional API keys for optional resources:</p> <ul> <li>BioPortal account (for grounding). The BioPortal key is necessary for using ontologies from BioPortal. You may get a key by signing up for an account on their web site.</li> <li>NCBI E-utilities. The NCBI email address and API key are used for retrieving text and metadata from PubMed. You may still access these resources without identifying yourself, but you may encounter rate limiting and errors.</li> <li>HuggingFace Hub. This API key is necessary to retrieve models from the HuggingFace Hub service.</li> </ul> <p>These optional keys may be set as follows:</p> <pre><code>poetry run runoak set-apikey -e bioportal &lt;your bioportal api key&gt;\npoetry run runoak set-apikey -e ncbi-email &lt;your email address&gt;\npoetry run runoak set-apikey -e ncbi-key &lt;your NCBI api key&gt;\npoetry run runoak set-apikey -e hfhub-key &lt;your HuggingFace Hub api key&gt;\n</code></pre>"},{"location":"#setup","title":"Setup","text":"<p>For feature development and contributing to the package:</p> <pre><code>git clone https://github.com/monarch-initiative/ontogpt.git\ncd ~/path/to/ontogpt\npoetry install\n</code></pre> <p>To simply start using the package in your workspace:</p> <pre><code>pip install ontogpt\n</code></pre> <p>Note that some features require installing additional, optional dependencies.</p> <p>These may be installed as:</p> <pre><code>poetry install --extras extra_name\n# OR\npip install ontogpt[extra_name]\n</code></pre> <p>where <code>extra_name</code> is one of the following:</p> <ul> <li><code>docs</code> - dependencies for building documentation</li> <li><code>web</code> - dependencies for the web application</li> <li><code>recipes</code> - dependencies for recipe scraping and parsing</li> <li><code>gpt4all</code> - dependencies for loading LLMs from GPT4All</li> <li><code>textract</code> - the textract plugin</li> <li><code>huggingface</code> - dependencies for accessing LLMs from HuggingFace Hub, remotely or locally</li> </ul>"},{"location":"#examples","title":"Examples","text":""},{"location":"#strategy-1-knowledge-extraction-using-spires","title":"Strategy 1: Knowledge extraction using SPIRES","text":""},{"location":"#input","title":"Input","text":"<p>Consider some text from one of the input files being used in the ontogpt test suite. You can find the text file here. You can download the raw file from the GitHub link to that input text file, or copy its contents over into another file, say, <code>abstract.txt</code>. An excerpt </p> <p>The cGAS/STING-mediated DNA-sensing signaling pathway is crucial   for interferon (IFN) production and host antiviral   responses</p> <p>... [snip] ...</p> <p>The underlying mechanism was the   interaction of US3 with \u03b2-catenin and its hyperphosphorylation of   \u03b2-catenin at Thr556 to block its nuclear translocation ... ...</p> <p>We can extract knowledge from the above text this into the GO pathway datamodel by running the following command:</p>"},{"location":"#command","title":"Command","text":"<pre><code>ontogpt extract -t gocam.GoCamAnnotations -i ~/path/to/abstract.txt\n</code></pre> <p>Note: The value accepted by the <code>-t</code> / <code>--template</code> argument is the base name of one of the LinkML schema / data model which can be found in the templates folder.</p>"},{"location":"#output","title":"Output","text":"<p>The output returned from the above command can be optionally redirected into an output file using the <code>-o</code> / <code>--output</code>.</p> <p>The following is a small part of what the larger schema-compliant output looks like:</p> <pre><code>genes:\n- HGNC:2514\n- HGNC:21367\n- HGNC:27962\n- US3\n- FPLX:Interferon\n- ISG\ngene_gene_interactions:\n- gene1: US3\n  gene2: HGNC:2514\ngene_localizations:\n- gene: HGNC:2514\n  location: Nuclear\ngene_functions:\n- gene: HGNC:2514\n  molecular_activity: Transcription\n- gene: HGNC:21367\n  molecular_activity: Production\n...\n</code></pre>"},{"location":"#working-mechanism","title":"Working Mechanism","text":"<ol> <li>You provide an arbitrary data model, describing the structure you want to extract text into. This can be nested (but see limitations below)</li> <li>Provide your preferred annotations for grounding <code>NamedEntity</code> fields</li> <li> <p>OntoGPT will:</p> </li> <li> <p>Generate a prompt</p> </li> <li>Feed the prompt to a language model (currently OpenAI GPT models)</li> <li>Parse the results into a dictionary structure</li> <li>Ground the results using a preferred annotator</li> </ol>"},{"location":"#gene-enrichment-using-spindoctor","title":"Gene Enrichment using SPINDOCTOR","text":"<p>Given a set of genes, OntoGPT can find similarities among them.</p> <p>Ex.:</p> <pre><code>ontogpt enrichment -U tests/input/genesets/sensory-ataxia.yaml\n</code></pre> <p>The default is to use ontological gene function synopses (via the Alliance API).</p> <ul> <li>To use narrative/RefSeq summaries, use the <code>--no-ontological-synopses</code> flag</li> <li>To run without any gene descriptions, use the <code>--no-annotations</code> flag</li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#define-your-own-extraction-model-using-linkml","title":"Define your own extraction model using LinkML","text":"<p>There are a number of pre-defined LinkML data models already developed here - src/ontogpt/templates/ which you can use as reference when creating your own data models.</p> <p>Define a schema (using a subset of LinkML) that describes the structure in which you want to extract knowledge from your text.</p> <p>```yaml   classes:     MendelianDisease:       attributes:         name:           description: the name of the disease           examples:             - value: peroxisome biogenesis disorder           identifier: true  ## needed for inlining         description:           description: a description of the disease           examples:             - value: &gt;-               Peroxisome biogenesis disorders, Zellweger syndrome spectrum (PBD-ZSS) is a group of autosomal recessive disorders affecting the formation of functional peroxisomes, characterized by sensorineural hearing loss, pigmentary retinal degeneration, multiple organ dysfunction and psychomotor impairment         synonyms:           multivalued: true           examples:             - value: Zellweger syndrome spectrum             - value: PBD-ZSS         subclass_of:           multivalued: true           range: MendelianDisease           examples:             - value: lysosomal disease             - value: autosomal recessive disorder         symptoms:           range: Symptom           multivalued: true           examples:             - value: sensorineural hearing loss             - value: pigmentary retinal degeneration         inheritance:           range: Inheritance           examples:             - value: autosomal recessive         genes:           range: Gene           multivalued: true           examples:             - value: PEX1             - value: PEX2             - value: PEX3</p> <pre><code>Gene:\n  is_a: NamedThing\n  id_prefixes:\n    - HGNC\n  annotations:\n    annotators: gilda:, bioportal:hgnc-nr\n\nSymptom:\n  is_a: NamedThing\n  id_prefixes:\n    - HP\n  annotations:\n    annotators: sqlite:obo:hp\n\nInheritance:\n  is_a: NamedThing\n  annotations:\n    annotators: sqlite:obo:hp\n```\n</code></pre> <ul> <li>Prompt hints can be specified using the <code>prompt</code> annotation (otherwise description is used)</li> <li>Multivalued fields are supported</li> <li>The default range is string \u2014 these are not grounded. Ex.: disease name, synonyms</li> <li>Define a class for each <code>NamedEntity</code></li> <li>For any <code>NamedEntity</code>, you can specify a preferred annotator using the <code>annotators</code> annotation</li> </ul> <p>We recommend following an established schema like BioLink Model, but you can define your own.</p> <p>Next step is to compile the schema. For that, you should place the schema YAML in the directory src/ontogpt/templates/. Then, run the <code>make</code> command at the top level. This will compile the schema to Python (Pydantic classes).</p> <p>Once you have defined your own schema / data model and placed in the correct directory, you can run the <code>extract</code> command. </p> <p>Ex.:</p> <pre><code>ontogpt extract -t mendelian_disease.MendelianDisease -i marfan-wikipedia.txt\n</code></pre>"},{"location":"#multiple-levels-of-nesting","title":"Multiple levels of nesting","text":"<p>Currently no more than two levels of nesting are recommended.</p> <p>If a field has a range which is itself a class and not a primitive, it will attempt to nest.</p> <p>Ex. the <code>gocam</code> schema has an attribute:</p> <pre><code>  attributes:\n      ...\n      gene_functions:\n        description: semicolon-separated list of gene to molecular activity relationships\n        multivalued: true\n        range: GeneMolecularActivityRelationship\n</code></pre> <p>The range <code>GeneMolecularActivityRelationship</code> has been specified inline, so it will nest.</p> <p>The generated prompt is:</p> <pre><code>gene_functions : &lt;semicolon-separated list of gene to molecular activities relationships&gt;\n</code></pre> <p>The output of this is then passed through further SPIRES iterations.</p>"},{"location":"#text-length-limit","title":"Text length limit","text":"<p>Currently SPIRES must use text-davinci-003, which has a total 4k token limit (prompt + completion).</p> <p>You can pass in a parameter to split the text into chunks. Returned results will be recombined automatically, but more experiments need to be done to determined how reliable this is.</p>"},{"location":"#schema-tips","title":"Schema tips","text":"<p>It helps to have an understanding of the LinkML schema language, but it should be possible to define your own schemas using the examples in src/ontogpt/templates as a guide.</p> <p>OntoGPT-specific extensions are specified as annotations.</p> <p>You can specify a set of annotators for a field using the <code>annotators</code> annotation.</p> <p>Ex.:</p> <pre><code>  Gene:\n    is_a: NamedThing\n    id_prefixes:\n      - HGNC\n    annotations:\n      annotators: gilda:, bioportal:hgnc-nr, obo:pr\n</code></pre> <p>The annotators are applied in order.</p> <p>Additionally, when performing grounding, the following measures can be taken to improve accuracy:</p> <ul> <li>Specify the valid set of ID prefixes using <code>id_prefixes</code></li> <li>Some vocabularies have structural IDs that are amenable to regexes, you can specify these using <code>pattern</code></li> <li>You can make use of <code>values_from</code> slot to specify a Dynamic Value Set</li> <li>For example, you can constrain the set of valid locations for a gene product to be subclasses of <code>cellular_component</code> in GO or <code>cell</code> in CL</li> </ul> <p>Ex.:</p> <pre><code>classes:\n  ...\n  GeneLocation:\n    is_a: NamedEntity\n    id_prefixes:\n      - GO\n      - CL\n    annotations:\n      annotators: \"sqlite:obo:go, sqlite:obo:cl\"\n    slot_usage:\n      id:\n        values_from:\n          - GOCellComponentType\n          - CellType\n\nenums:\n  GOCellComponentType:\n    reachable_from:\n      source_ontology: obo:go\n      source_nodes:\n        - GO:0005575 ## cellular_component\n  CellType:\n    reachable_from:\n      source_ontology: obo:cl\n      source_nodes:\n        - CL:0000000 ## cell\n</code></pre>"},{"location":"#owl-exports","title":"OWL Exports","text":"<p>The <code>extract</code> command will let you export the results as OWL axioms, utilizing linkml-owl mappings in the schema.</p> <p>Ex.:</p> <pre><code>ontogpt extract -t recipe -i recipe-spaghetti.txt -o recipe-spaghetti.owl -O owl\n</code></pre> <p>src/ontogpt/templates/recipe.yaml is an example schema that uses linkml-owl mappings.</p> <p>See the Makefile for a full pipeline that involves using robot to extract a subset of FOODON and merge in the extracted results. This uses recipe-scrapers.</p> <p>OWL output: recipe-all-merged.owl</p> <p>Classification:</p> <p></p>"},{"location":"#web-application-setup","title":"Web Application Setup","text":"<p>There is a bare bones web application for running OntoGPT and viewing results.</p> <p>Install the required dependencies by running the following command:</p> <pre><code>poetry install -E web\n</code></pre> <p>Then run this command to start the web application:</p> <pre><code>poetry run web-ontogpt\n</code></pre> <p>Note: The agent running uvicorn must have the API key set, so for obvious reasons don't host this publicly without authentication, unless you want your credits drained.</p>"},{"location":"#ontogpt-limitations","title":"OntoGPT Limitations","text":"<ol> <li>Non-deterministic</li> <li>This relies on an existing LLM, and LLMs can be fickle in their responses</li> <li>Coupled to OpenAI</li> <li>You will need an OpenAI account to use their API. In theory any LLM can be used but in practice the parser is tuned for OpenAI's models</li> </ol>"},{"location":"#spindoctor-web-app","title":"SPINDOCTOR web app","text":"<p>To start:</p> <pre><code>poetry run streamlit run src/ontogpt/streamlit/spindoctor.py\n</code></pre>"},{"location":"#huggingface-hub","title":"HuggingFace Hub","text":"<p>A select number of LLMs may be accessed through HuggingFace Hub. See the full list using <code>ontogpt list-models</code></p> <p>Specify a model name with the <code>-m</code> option.</p> <p>Example:</p> <pre><code>ontogpt extract -t mendelian_disease.MendelianDisease -i tests/input/cases/mendelian-disease-sly.txt -m FLAN_T5_BASE\n</code></pre>"},{"location":"#using-local-models","title":"Using local models","text":"<p>OntoGPT supports using language models released by GPT4All.</p> <p>Specify the name of a model when using the <code>extract</code> command with the <code>-m</code> or <code>--model</code> option and OntoGPT will retrieve the model.</p> <p>For example:</p> <pre><code>ontogpt --verbose extract -t mendelian_disease.MendelianDisease -i mendelian-disease-sly.txt -m ggml-gpt4all-j-v1.3-groovy\n</code></pre> <p>will download the <code>ggml-gpt4all-j-v1.3-groovy.bin</code> file, generate a prompt, and try that prompt against the specified model.</p>"},{"location":"#citation","title":"Citation","text":"<p>SPIRES is described further in: Caufield JH, Hegde H, Emonet V, Harris NL, Joachimiak MP, Matentzoglu N, et al. Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. </p> <p>arXiv publication: http://arxiv.org/abs/2304.02711</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions on recipes to test welcome from anyone! Just make a PR here. See this list for accepted URLs</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We gratefully acknowledge Bosch Research for their support of this research project.</p>"}]}